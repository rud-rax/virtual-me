
# Setup and Installation Phase

1. Install python3.12 

2. Create a virtual environment

```
python3 -m venv .venv
```

2. Activate the virtual env

```
source .venv/bin/activate
```

3. Install all dependencies for the project inside the virtual environment 

    - Langflow : [Click here for more info](https://docs.langflow.org)

```
pip3 install langflow --pre --force-reinstall
```

# Data Preparation

Data preparation for Retrieval Augmentation Generation (RAG) involves several key steps to ensure the effectiveness of the model in generating relevant and coherent responses. RAG is a method that combines retrieval-based and generation-based approaches in natural language processing tasks, such as question answering and dialogue systems. Here's a summary of the data preparation process for RAG:

1. **Data Collection**: The first step is to gather a diverse and comprehensive dataset relevant to the task at hand. This dataset typically includes pairs of queries or prompts and their corresponding responses or answers.

2. **Retrieval Corpus Construction**: A large corpus of text data is collected and preprocessed to serve as the retrieval corpus. This corpus contains a wide range of documents or passages that the model can search through to find relevant information.

3. **Indexing**: The retrieval corpus is indexed using efficient data structures like inverted indexes or approximate nearest neighbor search algorithms. This allows for fast and scalable retrieval of relevant documents or passages during inference.

4. **Preprocessing**: Both the queries/prompts and the retrieval corpus undergo preprocessing steps such as tokenization, lowercasing, and removing stopwords and punctuation. This ensures consistency and standardization in the data.

5. **Embedding Generation**: The queries/prompts and retrieval corpus are converted into dense vector representations using techniques like word embeddings (e.g., Word2Vec, GloVe) or contextual embeddings (e.g., BERT, GPT). These embeddings capture the semantic meaning of the text, enabling effective comparison and retrieval.

6. **Negative Sampling**: To train the model effectively, negative samples are generated by pairing queries/prompts with irrelevant responses or answers. This helps the model learn to distinguish between relevant and irrelevant information during training.

7. **Training Data Generation**: The final training data is created by combining the positive samples (pairs of queries and relevant responses) with the negative samples. This balanced dataset is used to train the RAG model.

8. **Validation and Testing Data**: Separate datasets are prepared for validation and testing to evaluate the performance of the trained model on unseen data. These datasets follow a similar format to the training data but are kept distinct to ensure unbiased evaluation.

By following these steps, researchers and practitioners can prepare high-quality data for training and evaluating Retrieval Augmentation Generation models, enabling them to achieve state-of-the-art performance in various natural language processing tasks.

# Building VectorDB

1. Create an account on datastax for VectorDB. [Click here](https://www.datastax.com)

2. Create a serverless vector DB on Astra. 

# Prototype Phase

1. Create a Langflow chain

```
langflow run
```

1. develop a prompt / instruction for the LLM to build an assistant (virtual bot of the person)

2. build a prototype virtual bot with default settings and attach resume of person as knowledge base

3. (optional) design a software architecture for backend 


# Backend Development Phase

1. develop the backend of application in python to interact with the virtual bot (based on software architecture)

2. (optional) design a software architecture for frontend


# Research Phase (API)

1. try out different APIs (like claude , bard , etc) for the backend application

2. develop the backend based on the API being used


# Research Phase (Virtual Bot)

1. try to use different prompting techniques to get better replies from the virtual bot

2. use different model parameters (temperature , max_length , top p ,etc )to get the desired answer for questions




